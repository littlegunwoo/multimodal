{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1idZ5F7WYZAQguKGJq5dEPTNkM5euXMYZ",
      "authorship_tag": "ABX9TyPqgeUz3pESffBy9T95OE4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlegunwoo/multimodal/blob/main/Mutimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "5mj4xf3tJEnn",
        "outputId": "8390ebef-505c-48e1-e8bc-8ae7efcc4705"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jJQ4GlS8JQUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVqD4Na0Mwp7",
        "outputId": "e8776eeb-ebf0-4f49-8ad5-bd6f495f6bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train集导入了 340 张图像\n",
            "Test集导入了 212 张图像\n",
            "Validation集导入了 150 张图像\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image  # PIL库用于图像处理\n",
        "\n",
        "\n",
        "# 设置train、test和validation目录路径\n",
        "train_directory = \"/content/drive/MyDrive/Colab Notebooks/image /train\"\n",
        "test_directory = \"/content/drive/MyDrive/Colab Notebooks/image /test\"\n",
        "validation_directory =\"/content/drive/MyDrive/Colab Notebooks/image /val\"\n",
        "\n",
        "# 存储图像和标签的列表\n",
        "train_images = []\n",
        "train_labels = []\n",
        "test_images = []\n",
        "test_labels = []\n",
        "validation_images = []\n",
        "validation_labels = []\n",
        "\n",
        "# 导入train目录下的FALSE数据集\n",
        "train_false_directory = \"/content/drive/MyDrive/Colab Notebooks/image /train/FALSE\"\n",
        "for filename in os.listdir(train_false_directory):\n",
        "    filepath = os.path.join(train_false_directory, filename)\n",
        "    image = Image.open(filepath)\n",
        "    train_images.append(image)\n",
        "    train_labels.append(0)  # 标签为0，因为在train_false目录下\n",
        "\n",
        "# 导入train目录下的TRUE数据集\n",
        "train_true_directory = \"/content/drive/MyDrive/Colab Notebooks/image /train/TRUE\"\n",
        "for filename in os.listdir(train_true_directory):\n",
        "    filepath = os.path.join(train_true_directory, filename)\n",
        "    image = Image.open(filepath)\n",
        "    train_images.append(image)\n",
        "    train_labels.append(1)  # 标签为1，因为在train_true目录下\n",
        "\n",
        "# 导入test目录下的FALSE数据集\n",
        "test_false_directory = \"/content/drive/MyDrive/Colab Notebooks/image /test/FALSE\"\n",
        "for filename in os.listdir(test_false_directory):\n",
        "    filepath = os.path.join(test_false_directory, filename).strip()\n",
        "    image = Image.open(filepath)\n",
        "    test_images.append(image)\n",
        "    test_labels.append(0)  # 标签为0，因为在test_false目录下\n",
        "\n",
        "# 导入test目录下的TRUE数据集\n",
        "test_true_directory =\"/content/drive/MyDrive/Colab Notebooks/image /test/TRUE\"\n",
        "for filename in os.listdir(test_true_directory):\n",
        "    filepath = os.path.join(test_true_directory, filename)\n",
        "    image = Image.open(filepath)\n",
        "    test_images.append(image)\n",
        "    test_labels.append(1)  # 标签为1，因为在test_true目录下\n",
        "\n",
        "# 导入validation目录下的FALSE数据集\n",
        "validation_false_directory = \"/content/drive/MyDrive/Colab Notebooks/image /val/FALSE\"\n",
        "for filename in os.listdir(validation_false_directory):\n",
        "    filepath = os.path.join(validation_false_directory, filename)\n",
        "    image = Image.open(filepath)\n",
        "    validation_images.append(image)\n",
        "    validation_labels.append(0)  # 标签为0，因为在validation_false目录下\n",
        "\n",
        "# 导入validation目录下的TRUE数据集\n",
        "validation_true_directory = \"/content/drive/MyDrive/Colab Notebooks/image /val/TRUE\"\n",
        "for filename in os.listdir(validation_true_directory):\n",
        "    filepath = os.path.join(validation_true_directory, filename)\n",
        "    image = Image.open(filepath)\n",
        "    validation_images.append(image)\n",
        "    validation_labels.append(1)  # 标签为1，因为在validation_true目录下\n",
        "\n",
        "# 打印导入的图像数量\n",
        "print(\"Train集导入了\", len(train_images), \"张图像\")\n",
        "print(\"Test集导入了\", len(test_images), \"张图像\")\n",
        "print(\"Validation集导入了\", len(validation_images), \"张图像\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 设置图像生成器的参数\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # 将像素值缩放到 [0, 1] 区间\n",
        "    rotation_range=40,      # 随机旋转角度范围\n",
        "    width_shift_range=0.2,  # 随机水平平移的范围\n",
        "    height_shift_range=0.2, # 随机垂直平移的范围\n",
        "    shear_range=0.2,        # 随机错切变换的范围\n",
        "    zoom_range=0.2,         # 随机缩放范围\n",
        "    horizontal_flip=True,   # 随机水平翻转\n",
        "    fill_mode='nearest'     # 填充像素的方式\n",
        ")\n"
      ],
      "metadata": {
        "id": "IWW71eG4LcyB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "batch_size=32\n",
        "image_height=224\n",
        "image_width=224\n",
        "# Create training set generator and apply parameters\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    train_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',  # Use 'training' subset\n",
        "    classes=['FALSE', 'TRUE']  # Explicitly specify classes\n",
        ")\n",
        "\n",
        "# Create test set generator and apply parameters\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    test_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['FALSE', 'TRUE']  # Explicitly specify classes\n",
        ")\n",
        "\n",
        "# Create validation set generator and apply parameters\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    validation_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['FALSE', 'TRUE']  # Explicitly specify classes\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XW2JHQVmAH",
        "outputId": "c6a2f71c-3f61-4986-d3fd-e80517cfb70a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 340 images belonging to 2 classes.\n",
            "Found 212 images belonging to 2 classes.\n",
            "Found 150 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# 统一图像大小\n",
        "desired_size = (224, 224)\n",
        "\n",
        "# 调整训练集中的图像大小\n",
        "resized_train_images = []\n",
        "for image in train_images:\n",
        "    resized_image = image.resize(desired_size, Image.ANTIALIAS)\n",
        "    resized_train_images.append(resized_image)\n",
        "\n",
        "# 调整测试集中的图像大小\n",
        "resized_test_images = []\n",
        "for image in test_images:\n",
        "    resized_image = image.resize(desired_size, Image.ANTIALIAS)\n",
        "    resized_test_images.append(resized_image)\n",
        "\n",
        "# 调整验证集中的图像大小\n",
        "resized_validation_images = []\n",
        "for image in validation_images:\n",
        "    resized_image = image.resize(desired_size, Image.ANTIALIAS)\n",
        "    resized_validation_images.append(resized_image)\n",
        "\n",
        "# 将调整大小后的图像转换为 NumPy 数组\n",
        "train_images_array = np.array([np.array(img) for img in resized_train_images])\n",
        "test_images_array = np.array([np.array(img) for img in resized_test_images])\n",
        "validation_images_array = np.array([np.array(img) for img in resized_validation_images])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbvlgWT64e1u",
        "outputId": "c67fcf80-9084-4d08-f136-05cd3aa4210a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7d3fc768545c>:9: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  resized_image = image.resize(desired_size, Image.ANTIALIAS)\n",
            "<ipython-input-6-7d3fc768545c>:15: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  resized_image = image.resize(desired_size, Image.ANTIALIAS)\n",
            "<ipython-input-6-7d3fc768545c>:21: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  resized_image = image.resize(desired_size, Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load ResNet50 base model with pre-trained weights\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers[:-10]:  # 解冻最后10层预训练层\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add your own layers on top of the base model\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)  # 增加全连接层的节点数量\n",
        "x = layers.Dropout(0.5)(x)  # 添加Dropout层以减少过拟合\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model with binary crossentropy loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9ofxqszPeuJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c65a29-dc1f-4f88-ddbc-0f29e44fdd06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**文本**"
      ],
      "metadata": {
        "id": "36zxl42qu4J6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYHttjKgS5g4",
        "outputId": "a77dfd86-d24f-4891-c066-48b2d430f40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 351 files belonging to 2 classes.\n",
            "Found 151 files belonging to 2 classes.\n",
            "Found 214 files belonging to 2 classes.\n",
            "Input shape: (32, 600)\n",
            "Input sequence: tf.Tensor(\n",
            "[382   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0], shape=(600,), dtype=int64)\n",
            "Label: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import random\n",
        "from tensorflow import keras\n",
        "\n",
        "# 设置批处理大小\n",
        "batch_size = 32\n",
        "\n",
        "# 设置基础目录\n",
        "base_dir = pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/Fakehealthnews\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "# 从目录中创建数据集\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Fakehealthnews/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Fakehealthnews/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/Fakehealthnews/test\", batch_size=batch_size\n",
        ")\n",
        "# 创建 TextVectorization 层\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "# 对文本数据进行适应（adapt）\n",
        "text_vectorization.adapt(train_ds.map(lambda x, y: x))\n",
        "\n",
        "# 创建整数序列数据集\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "# 查看数据集的一个 batch\n",
        "for x_batch, y_batch in int_train_ds.take(1):\n",
        "    print(\"Input shape:\", x_batch.shape)  # 输出形状\n",
        "    print(\"Input sequence:\", x_batch[0])   # 输出第一个样本的整数序列\n",
        "    print(\"Label:\", y_batch[0])             # 输出第一个样本的标签\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uln6pGtAbAHg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxglV-qwbAHh"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyUp-BKwbAHi",
        "outputId": "43e2143f-bca0-44c6-dc85-0c21a604b468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         5273600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5817633 (22.19 MB)\n",
            "Trainable params: 5817633 (22.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 93s 8s/step - loss: 1.6901 - accuracy: 0.4758 - val_loss: 0.6959 - val_accuracy: 0.5166\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.9081 - accuracy: 0.5670 - val_loss: 0.7493 - val_accuracy: 0.4834\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 55s 5s/step - loss: 0.7853 - accuracy: 0.6382 - val_loss: 0.7542 - val_accuracy: 0.4834\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 60s 5s/step - loss: 0.4529 - accuracy: 0.7721 - val_loss: 1.8018 - val_accuracy: 0.4834\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.2285 - accuracy: 0.9259 - val_loss: 1.0824 - val_accuracy: 0.4834\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.0580 - accuracy: 0.9886 - val_loss: 2.4407 - val_accuracy: 0.4834\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 57s 5s/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 1.9612 - val_accuracy: 0.4834\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 60s 5s/step - loss: 0.0064 - accuracy: 0.9972 - val_loss: 2.3951 - val_accuracy: 0.4834\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 55s 5s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.5745 - val_accuracy: 0.4834\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 56s 5s/step - loss: 9.4312e-04 - accuracy: 1.0000 - val_loss: 2.5687 - val_accuracy: 0.4834\n",
            "7/7 [==============================] - 36s 1s/step - loss: 0.7039 - accuracy: 0.5187\n",
            "Test acc: 0.519\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**多模态**"
      ],
      "metadata": {
        "id": "uMf7KolQwC7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Embedding, GlobalAveragePooling1D, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "import pathlib\n",
        "\n",
        "# 设置批处理大小和图像尺寸\n",
        "batch_size = 32\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "\n",
        "# 设置文本处理参数\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "# 图像数据生成器\n",
        "image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 文本数据路径\n",
        "text_data_dir = \"/content/drive/MyDrive/Colab Notebooks/Fakehealthnews/train\"\n",
        "\n",
        "# 图像数据生成器\n",
        "image_train_generator = image_datagen.flow_from_directory(\n",
        "    train_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',  # Use 'training' subset\n",
        "    classes=['FALSE', 'TRUE']  # Explicitly specify classes\n",
        ")\n",
        "\n",
        "# 文本数据集\n",
        "text_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    text_data_dir,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# 文本数据处理层\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "# 对文本数据进行适应\n",
        "text_vectorization.adapt(text_train_ds.map(lambda x, y: x))\n",
        "\n",
        "# 创建文本数据的整数序列数据集\n",
        "int_text_train_ds = text_train_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oIqhTS3wFSO",
        "outputId": "627bb0c6-ce8a-4cc0-f6fd-99f8d28a970d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 340 images belonging to 2 classes.\n",
            "Found 351 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义训练集的图像数据生成器\n",
        "train_image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_image_generator = train_image_datagen.flow_from_directory(\n",
        "    train_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training',  # Use 'training' subset\n",
        "    classes=['FALSE', 'TRUE'],  # Explicitly specify classes\n",
        "    seed=1  # 设置seed值\n",
        ")\n",
        "\n",
        "# 定义验证集的图像数据生成器\n",
        "val_image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_image_generator = val_image_datagen.flow_from_directory(\n",
        "    validation_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['FALSE', 'TRUE'],  # Explicitly specify classes\n",
        "    seed=1  # 设置相同的seed值\n",
        ")\n",
        "\n",
        "# 定义测试集的图像数据生成器\n",
        "test_image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_image_generator = test_image_datagen.flow_from_directory(\n",
        "    test_directory,\n",
        "    target_size=(image_height, image_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    classes=['FALSE', 'TRUE'],  # Explicitly specify classes\n",
        "    seed=1  # 设置相同的seed值\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy2gBJfCyBuJ",
        "outputId": "0e7d9c0f-f008-4978-a307-d597def67acd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 340 images belonging to 2 classes.\n",
            "Found 150 images belonging to 2 classes.\n",
            "Found 212 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_generator(image_generator, text_dataset):\n",
        "    while True:\n",
        "        # 生成图像数据\n",
        "        images, labels = next(image_generator)\n",
        "        # 生成文本数据\n",
        "        texts, labels = next(iter(text_dataset))\n",
        "        # 将图像数据和文本数据整合成一个元组作为输出\n",
        "        yield (images, texts), labels\n",
        "\n",
        "# 使用自定义的多模态数据生成器\n",
        "train_multimodal_generator = multimodal_generator(train_image_generator, int_train_ds)\n",
        "val_multimodal_generator = multimodal_generator(val_image_generator, int_val_ds)\n",
        "test_multimodal_generator = multimodal_generator(test_image_generator, int_test_ds)\n"
      ],
      "metadata": {
        "id": "l5goT7eTzb4G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Embedding, GlobalAveragePooling1D, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 图像数据路径\n",
        "train_directory = \"/content/drive/MyDrive/Colab Notebooks/image /train\"\n",
        "\n",
        "# 图像数据输入\n",
        "image_input = Input(shape=(image_height, image_width, 3))\n",
        "conv1 = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
        "pool1 = MaxPooling2D((2, 2))(conv1)\n",
        "conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2))(conv2)\n",
        "flatten = Flatten()(pool2)\n",
        "dense_image = Dense(64, activation='relu')(flatten)\n",
        "\n",
        "# 文本数据输入\n",
        "text_input = Input(shape=(max_length,))\n",
        "embedding_layer = Embedding(input_dim=max_tokens, output_dim=128)(text_input)\n",
        "average_pooling = GlobalAveragePooling1D()(embedding_layer)\n",
        "dense_text = Dense(64, activation='relu')(average_pooling)\n",
        "\n",
        "# 添加一个与图像部分相同维度的全连接层\n",
        "dense_text = Dense(64, activation='relu')(dense_text)\n",
        "\n",
        "# 将图像和文本数据合并\n",
        "concatenated = concatenate([dense_image, dense_text])\n",
        "\n",
        "# 输出层\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# 构建模型\n",
        "model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "\n",
        "# 编译模型\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 查看模型结构\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rb2QfRGwOKJ",
        "outputId": "a04c98fe-542f-44a8-ee84-4d64ce047cd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 222, 222, 32)         896       ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)         0         ['conv2d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 600)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)         18496     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 600, 128)             2560000   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)           0         ['conv2d_1[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 128)                  0         ['embedding[0][0]']           \n",
            " GlobalAveragePooling1D)                                                                          \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 186624)               0         ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 64)                   8256      ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 64)                   1194400   ['flatten[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 64)                   4160      ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 128)                  0         ['dense_2[0][0]',             \n",
            "                                                                     'dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1)                    129       ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14535937 (55.45 MB)\n",
            "Trainable params: 14535937 (55.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "history = model.fit(\n",
        "    train_multimodal_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_image_generator),\n",
        "    validation_data=val_multimodal_generator,\n",
        "    validation_steps=len(val_image_generator)\n",
        ")\n",
        "\n",
        "# 在测试集上评估模型\n",
        "test_loss, test_accuracy = model.evaluate(test_multimodal_generator, steps=len(test_image_generator))\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yNATOVlu-Llu",
        "outputId": "0cab712f-313e-42a9-be66-e4ba19b4a3f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/11 [==========================>...] - ETA: 4s - loss: 2.4868 - accuracy: 0.5656"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node model_1/concatenate/concat defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-15-d0a0ade1768f>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/base_merge.py\", line 196, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/concatenate.py\", line 134, in _merge_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3580, in concatenate\n\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [20,64] vs. shape[1] = [32,64]\n\t [[{{node model_1/concatenate/concat}}]] [Op:__inference_train_function_6476]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d0a0ade1768f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_multimodal_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model_1/concatenate/concat defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-15-d0a0ade1768f>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/base_merge.py\", line 196, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/concatenate.py\", line 134, in _merge_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 3580, in concatenate\n\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [20,64] vs. shape[1] = [32,64]\n\t [[{{node model_1/concatenate/concat}}]] [Op:__inference_train_function_6476]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resnet50**"
      ],
      "metadata": {
        "id": "2XO3pMXN7OwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Embedding, GlobalAveragePooling2D, concatenate\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow as tf\n",
        "\n",
        "# 图像处理\n",
        "image_input = Input(shape=(image_height, image_width, 3))\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False)\n",
        "for layer in resnet_model.layers:\n",
        "    layer.trainable = False\n",
        "image_features = resnet_model(image_input)\n",
        "image_features = GlobalAveragePooling2D()(image_features)\n",
        "\n",
        "# 文本处理\n",
        "text_input = Input(shape=(32, 600))\n",
        "text_embedding = Embedding(input_dim=max_tokens, output_dim=128)(text_input)\n",
        "text_features = GlobalAveragePooling2D()(text_embedding)  # 使用 GlobalAveragePooling1D 处理文本特征\n",
        "\n",
        "# 合并图像和文本特征\n",
        "merged_features = concatenate([image_features, text_features])\n",
        "\n",
        "# 全连接层\n",
        "dense1 = Dense(64, activation='relu')(merged_features)\n",
        "output = Dense(1, activation='sigmoid')(dense1)\n"
      ],
      "metadata": {
        "id": "M5FLEZ9d7RdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 构建模型\n",
        "model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "\n",
        "# 编译模型\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 训练模型\n",
        "history = model.fit(\n",
        "    train_multimodal_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_image_generator),\n",
        "    validation_data=val_multimodal_generator,\n",
        "    validation_steps=len(val_image_generator)\n",
        ")\n",
        "\n",
        "# 在测试集上评估模型\n",
        "test_loss, test_accuracy = model.evaluate(test_multimodal_generator, steps=len(test_image_generator))\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "6B2gweh7BYkN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "outputId": "07411f03-f5a5-471b-e329-75336979d7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_6' (type Functional).\n    \n    Input 0 of layer \"global_average_pooling2d_8\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, None, 128)\n    \n    Call arguments received by layer 'model_6' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, None, None, None), dtype=float32)', 'tf.Tensor(shape=(None, None), dtype=int64)')\n      • training=True\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e221fdf70bf9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_multimodal_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_6' (type Functional).\n    \n    Input 0 of layer \"global_average_pooling2d_8\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, None, 128)\n    \n    Call arguments received by layer 'model_6' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, None, None, None), dtype=float32)', 'tf.Tensor(shape=(None, None), dtype=int64)')\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    }
  ]
}